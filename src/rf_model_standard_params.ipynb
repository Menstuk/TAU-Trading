{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "from utils.database import get_db\n",
    "import utils.models as models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create connection to Database and extraction of required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create connection to the database\n",
    "db = get_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define requested ticker ID and requested threshold\n",
    "ticker_id =\n",
    "model_threshold = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the presentation format of float numbers within the dataframe\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch \"daily multipliers\" table from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = db.query(models.FullDailyMultipliers)\n",
    "df_daily_multipliers = pd.read_sql(query.statement, query.session.bind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check the table was retrieved\n",
    "df_daily_multipliers.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the stock according to given stock id (= ticker_id)\n",
    "daily_multipliers = df_daily_multipliers.loc[df_daily_multipliers['stock_id'] == ticker_id]\n",
    "\n",
    "# Delete all dates < 2.1.13\n",
    "daily_multipliers = daily_multipliers.query('\"2013-01-01\" < date').copy().reset_index(drop=True)\n",
    "\n",
    "# Check the table is filtered correctly\n",
    "daily_multipliers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for missing values in table\n",
    "df_daily_multipliers[df_daily_multipliers.isna().any(axis=1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# In case there are missing values in the table, run the following preprocessing code (otherwise - can skip):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for null values - if found - complete null values using interpolation (up to 5 days in a row)\n",
    "\n",
    "# First, find Nan values in dataframe, save this as a mask:\n",
    "mask = daily_multipliers.isna()\n",
    "\n",
    "# Then, calculate the length of consecutive Nans in order to find Nans > 5, appoint each Nan that's found with its defining \"score\" :\n",
    "df_na_sizes = (mask.ne(mask.shift()).cumsum()\n",
    "                   .where(mask)\n",
    "                   .apply(lambda c: c.groupby(c).transform('size'))\n",
    "               )\n",
    "\n",
    "# In order to differentiate which rows have Nan values should be erased - rename the Nan values with score >=6 with the word \"Drop\":\n",
    "df_without_consec_nan = daily_multipliers.mask(df_na_sizes.ge(6), 'drop')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Next, erase all rows that have a \"Drop\" value in them:\n",
    "df_filtered = df_without_consec_nan[(df_without_consec_nan.iloc[:, 3:] != 'drop').all(axis=1)]\n",
    "\n",
    "df_filtered"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Finally, handle the remaining Nan values in dataframe by using interpolation:\n",
    "df_filtered = df_filtered.astype({\"trailing_peg_1_y\" : float})\n",
    "\n",
    "df_filtered.iloc[:, 3:] = df_filtered.iloc[:, 3:].interpolate(axis=0, limit_area='inside', limit=5)\n",
    "\n",
    "daily_multipliers = df_filtered"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check to make sure there are no Nan values in final dataframe\n",
    "daily_multipliers[daily_multipliers.isna().any(axis=1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch \"end of day prices\" table from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = db.query(models.EndOfDayPrices)\n",
    "df_end_of_day_prices = pd.read_sql(query.statement, query.session.bind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check the table was retrieved\n",
    "df_end_of_day_prices.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select specific stock from \"end of day prices\" table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the stock according to given stock id (= ticker_id)\n",
    "end_of_day_prices = df_end_of_day_prices.loc[df_end_of_day_prices['stock_id'] == ticker_id]\n",
    "\n",
    "# Delete all dates < 2.1.13\n",
    "end_of_day_prices = end_of_day_prices.query('\"2013-01-01\" < date').copy().reset_index(drop=True)\n",
    "\n",
    "# Check the table is filtered correctly\n",
    "end_of_day_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for missing values in table\n",
    "end_of_day_prices[end_of_day_prices.isna().any(axis=1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# In case there are missing values in the table, run the following preprocessing code (otherwise - can skip):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for null values - if found - complete null values using interpolation (up to 5 days in a row)\n",
    "\n",
    "# First, find Nan values in dataframe, save this as a mask:\n",
    "mask = end_of_day_prices.isna()\n",
    "\n",
    "# Then, calculate the length of consecutive Nans in order to find Nans > 5, appoint each Nan that's found with its defining \"score\" :\n",
    "df_na_sizes = (mask.ne(mask.shift()).cumsum()\n",
    "                   .where(mask)\n",
    "                   .apply(lambda c: c.groupby(c).transform('size'))\n",
    "               )\n",
    "\n",
    "# In order to differentiate which rows have Nan values should be erased - rename the Nan values with score >=6 with the word \"Drop\":\n",
    "df_without_consec_nan = end_of_day_prices.mask(df_na_sizes.ge(6), 'drop')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Next, erase all rows that have a \"Drop\" value in them:\n",
    "df_filtered = df_without_consec_nan[(df_without_consec_nan.iloc[:, 3:] != 'drop').all(axis=1)]\n",
    "\n",
    "df_filtered"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Finally, handle the remaining Nan values in dataframe by using interpolation:\n",
    "df_filtered = df_filtered.astype({\"trailing_peg_1_y\" : float})\n",
    "\n",
    "df_filtered.iloc[:, 3:] = df_filtered.iloc[:, 3:].interpolate(axis=0, limit_area='inside', limit=5)\n",
    "\n",
    "end_of_day_prices = df_filtered"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check to make sure there are no Nan values in final dataframe\n",
    "end_of_day_prices[end_of_day_prices.isna().any(axis=1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch \"pfree cash flow multiplier\" table from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = db.query(models.PFreeCashFlowMultiplier)\n",
    "df_pfree_cash_flow = pd.read_sql(query.statement, query.session.bind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check the table was retrieved\n",
    "df_end_of_day_prices.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Select specific stock from \"pfree cash flow multiplier\" table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select the stock according to given stock id (= ticker_id)\n",
    "pfree_cash_flow = df_pfree_cash_flow.loc[df_pfree_cash_flow['stock_id'] == ticker_id]\n",
    "\n",
    "# Delete all dates < 2.1.13\n",
    "pfree_cash_flow = pfree_cash_flow.query('\"2013-01-01\" < date').copy().reset_index(drop=True)\n",
    "\n",
    "# Check the table is filtered correctly\n",
    "pfree_cash_flow.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for missing values in table\n",
    "pfree_cash_flow[pfree_cash_flow.isna().any(axis=1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# In case there are missing values in the table, run the following preprocessing code (otherwise - can skip):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for null values - if found - complete null values using interpolation (up to 5 days in a row)\n",
    "\n",
    "# First, find Nan values in dataframe, save this as a mask:\n",
    "mask = pfree_cash_flow.isna()\n",
    "\n",
    "# Then, calculate the length of consecutive Nans in order to find Nans > 5, appoint each Nan that's found with its defining \"score\" :\n",
    "df_na_sizes = (mask.ne(mask.shift()).cumsum()\n",
    "                   .where(mask)\n",
    "                   .apply(lambda c: c.groupby(c).transform('size'))\n",
    "               )\n",
    "\n",
    "# In order to differentiate which rows have Nan values should be erased - rename the Nan values with score >=6 with the word \"Drop\":\n",
    "df_without_consec_nan = pfree_cash_flow.mask(df_na_sizes.ge(6), 'drop')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Next, erase all rows that have a \"Drop\" value in them:\n",
    "df_filtered = df_without_consec_nan[(df_without_consec_nan.iloc[:, 3:] != 'drop').all(axis=1)]\n",
    "\n",
    "df_filtered"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Finally, handle the remaining Nan values in dataframe by using interpolation:\n",
    "\n",
    "df_filtered = df_filtered.astype({\"pfree_cash_flow_ratio\" : float})\n",
    "\n",
    "df_filtered.iloc[:, 3:] = df_filtered.iloc[:, 3:].interpolate(axis=0, limit_area='inside', limit=5)\n",
    "\n",
    "pfree_cash_flow = df_filtered"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check if to make sure there are no Nan values in final dataframe\n",
    "pfree_cash_flow[pfree_cash_flow.isna().any(axis=1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch \"graham number\" table from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = db.query(models.GrahamNumber)\n",
    "df_graham_number = pd.read_sql(query.statement, query.session.bind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check the table was retrieved\n",
    "df_graham_number.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Select specific stock from \"graham number\" table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select the stock according to given stock id (= ticker_id)\n",
    "graham_number = df_graham_number.loc[df_graham_number['stock_id'] == ticker_id]\n",
    "\n",
    "# filter year >= 2012\n",
    "graham_number = graham_number.query('2012 < year').copy().reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for null values - if found any - \"graham number\" dataframe will not be used in final joined dataframe\n",
    "if graham_number['graham_value'].isnull().values.any():\n",
    "    print(\"'Graham Number' dataframe will not be joined to final dataframe due to missing values\")\n",
    "else:\n",
    "    print(\"'Graham Number' dataframe doesn't have missing values, can be joined to final dataframe\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch \"overview data\" table from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview = models.QuarterlyOverview\n",
    "query = db.query(overview.stock_id, overview.year, overview.quarter, overview.currentRatio )\n",
    "df_overview_data = pd.read_sql(query.statement, query.session.bind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check the table was retrieved\n",
    "df_overview_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Choose the stock_id we want to check (ticker_id)\n",
    "overview_data = df_overview_data.loc[df_overview_data['stock_id'] == ticker_id]\n",
    "\n",
    "# filter out year =< 2012 and quarter \"0\"\n",
    "overview_data = overview_data.query(\"2012 < year and quarter != 0\").copy().reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for null values - if found - \"overeview data\" dataframe will not be used in final joined dataframe\n",
    "if overview_data['currentRatio'].isnull().values.any():\n",
    "    print(\"'overeview data' dataframe will not be joined to final dataframe due to missing values\")\n",
    "else:\n",
    "    print(\"'overeview data' dataframe doesn't have missing values, can be joined to final dataframe\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join all dataframes to one final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, combine the two dataframes: \"end of day prices\" and \"daily multipliers\" into one joined dataframe (using outer join to maintain all rows)\n",
    "joined_dataframe = pd.merge(end_of_day_prices, daily_multipliers, on='date', how='inner')\n",
    "\n",
    "# Create 2 new columns for \"date_plus_3m\" & \"market_cap_plus_3m\"\n",
    "day_gap = 60\n",
    "date_plus_3m = joined_dataframe.loc[day_gap:,'date'].values\n",
    "market_cap_plus_3m = joined_dataframe.loc[day_gap:,'market_cap'].values\n",
    "\n",
    "\n",
    "joined_dataframe.drop(joined_dataframe.tail(day_gap).index, inplace = True)\n",
    "joined_dataframe['date_plus_3m'] = date_plus_3m\n",
    "joined_dataframe['market_cap_plus_3m'] = market_cap_plus_3m\n",
    "\n",
    "# Rename columns\n",
    "joined_dataframe.rename(columns = {'id_x': 'id', 'stock_id_x':'stock_id'}, inplace=True)\n",
    "\n",
    "# Setting the index column to column 'id'\n",
    "joined_dataframe['id'] = joined_dataframe.index\n",
    "\n",
    "# Dropping redundant columns\n",
    "joined_dataframe = joined_dataframe.drop(['stock_id_y', 'id_y'], axis=1)\n",
    "\n",
    "# Rename dataframe for clarity purposes\n",
    "multiplier_with_closing_prices = joined_dataframe\n",
    "\n",
    "# Check for Nan values in joined dataframe\n",
    "multiplier_with_closing_prices[multiplier_with_closing_prices.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second, join the two dataframes: \"multiplier_with_closing_prices\" and \"pfree_cash_flow\" into one joined dataframe (using outer join to maintain all rows)\n",
    "joined_dataframe = pd.merge(multiplier_with_closing_prices, pfree_cash_flow, on='date', how='left')\n",
    "\n",
    "# Rename columns\n",
    "joined_dataframe.rename(columns = {'id_x': 'id', 'stock_id_x':'stock_id'}, inplace=True)\n",
    "\n",
    "# Convert \"date\" column's type from object to datetime and \"stock id\" to int\n",
    "joined_dataframe['date'] = pd.to_datetime(joined_dataframe['date'])\n",
    "joined_dataframe['stock_id'] = joined_dataframe['stock_id'].astype('int')\n",
    "\n",
    "# Handle Nan values that were formed due to the left join of the two dataframes\n",
    "joined_dataframe[\"pfree_cash_flow_ratio\"] = joined_dataframe[\"pfree_cash_flow_ratio\"].interpolate(axis=0, limit_area='inside', limit=5)\n",
    "\n",
    "joined_dataframe[\"year\"] = joined_dataframe[\"date\"].dt.year\n",
    "joined_dataframe['quarter'] = joined_dataframe['date'].dt.quarter\n",
    "\n",
    "# Setting the index column to column 'id'\n",
    "joined_dataframe['id'] = joined_dataframe.index\n",
    "\n",
    "# Dropping redundant columns\n",
    "joined_dataframe = joined_dataframe.drop(['stock_id_y', 'id_y'], axis=1)\n",
    "\n",
    "# Rename dataframe for clarity purposes\n",
    "multiplier_with_closing_prices_and_cash_flow = joined_dataframe\n",
    "\n",
    "# Check for Nan values in joined dataframe\n",
    "multiplier_with_closing_prices_and_cash_flow[multiplier_with_closing_prices_and_cash_flow.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### If \"graham number\" dataframe is without Nan - run the following cell, if not skip it"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, merge the dataframe - \"graham number\", which has quarterly data with the previously joined dataframe,\n",
    "joined_dataframe = pd.merge(multiplier_with_closing_prices_and_cash_flow, graham_number, on=['year', 'quarter'], how='left')\n",
    "\n",
    "# Rename columns\n",
    "joined_dataframe.rename(columns = {'id_x': 'id', 'stock_id_x':'stock_id'}, inplace=True)\n",
    "\n",
    "# Dropping redundant columns\n",
    "joined_dataframe = joined_dataframe.drop(['stock_id_y', 'id_y'], axis=1)\n",
    "\n",
    "# Rename dataframe for clarity purposes\n",
    "four_joined_dataframes = joined_dataframe\n",
    "\n",
    "# Check for Nan values in joined dataframe\n",
    "four_joined_dataframes[four_joined_dataframes.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### If \"overview\" dataframe is without Nan - run one of the following possible cells, if not skip both of them"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. Run the following cell if \"graham number\" was joined to final joined dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Finally, merge the remaining dataframe - \"overview\", which has quarterly data with the previously joined dataframe,\n",
    "joined_dataframe = pd.merge(four_joined_dataframes, overview_data, on=['stock_id', 'year', 'quarter'], how='left')\n",
    "\n",
    "# Rename dataframe for clarity purposes\n",
    "final_joined_dataframe = joined_dataframe\n",
    "\n",
    "# Check for Nan values in joined dataframe\n",
    "final_joined_dataframe[final_joined_dataframe.isna().any(axis=1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Run the following cell if \"graham number\" was not joined to final joined dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Finally, merge the remaining dataframe - \"overview\", which has quarterly data with the previously joined dataframe,\n",
    "joined_dataframe = pd.merge(multiplier_with_closing_prices_and_cash_flow, overview_data, on=['stock_id', 'year', 'quarter'], how='left')\n",
    "\n",
    "# Rename dataframe for clarity purposes\n",
    "final_joined_dataframe = joined_dataframe\n",
    "\n",
    "# Check for Nan values in joined dataframe\n",
    "final_joined_dataframe[final_joined_dataframe.isna().any(axis=1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create new column: \"diff_in_market_cap_perc\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column - \"diff_in_market_cap_perc\" where:\n",
    "# calculates the change in market cap value 3 months apart and presents the difference in percentage\n",
    "\n",
    "col1 = final_joined_dataframe['market_cap']\n",
    "col2 = final_joined_dataframe['market_cap_plus_3m']\n",
    "\n",
    "final_joined_dataframe['diff_in_mc'] = col2.sub(col1, axis=0)\n",
    "final_joined_dataframe['diff_in_mc_perc'] = ((col2.sub(col1, axis=0)).div(col1)).mul(100)\n",
    "\n",
    "# Check to assure the final dataframe contains new column\n",
    "final_joined_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label threshold (change according to need)\n",
    "label_threshold = 1.5\n",
    "\n",
    "# create the label column (our \"y\" column, binary result according to our \"label_threshold\"% threshold) based on the price difference between 3 months\n",
    "\n",
    "final_joined_dataframe['label'] = (final_joined_dataframe[\"diff_in_price_perc\"] > label_threshold).astype(int)\n",
    "final_joined_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check for Nan values in the dataframe\n",
    "final_joined_dataframe[final_joined_dataframe.isna().any(axis=1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert \"date_plus_3m\" columns type from 'string' to 'datetime'\n",
    "\n",
    "final_joined_dataframe['date_plus_3m'] = pd.to_datetime(final_joined_dataframe['date_plus_3m'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_joined_dataframe.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Splitting the Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train: 67%, test: 23%\n",
    "train = final_joined_dataframe.query('\"2013-01-01\" < date <= \"2018-05-30\"').copy()\n",
    "test = final_joined_dataframe.query('\"2018-06-01\" < date <= \"2020-02-01\"').copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the x (=features) and y (=label) columns\n",
    "X_Cols = ['market_cap','enterprise_val','pe_ratio','pb_ratio','trailing_peg_1_y', 'pfree_cash_flow_ratio']\n",
    "Y_Cols = ['label']\n",
    "\n",
    "# # Split X and y into X_\n",
    "X_train, X_test = train[X_Cols], test[X_Cols]\n",
    "y_train, y_test = train[Y_Cols].values.ravel(), test[Y_Cols].values.ravel()\n",
    "\n",
    "# Create a Random Forest Classifier\n",
    "rand_frst_clf = RandomForestClassifier(n_estimators = 100, oob_score = True, criterion = \"gini\", random_state = 0)\n",
    "\n",
    "# Fit (train) the data to the model\n",
    "rand_frst_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions (test)\n",
    "y_pred_proba = rand_frst_clf.predict_proba(X_test)[:,1]\n",
    "# y_pred = rand_frst_clf.predict(X_test)\n",
    "y_pred = y_pred_proba > model_threshold"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print the Accuracy of our Model.\n",
    "print('Correct Prediction (%): ', accuracy_score(y_test, rand_frst_clf.predict(X_test), normalize = True) * 100.0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compute model report"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the target names\n",
    "target_names = ['Down Day', 'Up Day']\n",
    "\n",
    "# Build a classification report\n",
    "report = classification_report(y_true = y_test, y_pred = y_pred, target_names = target_names, output_dict = True)\n",
    "\n",
    "# Add it to a data frame, transpose it for readability.\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "report_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def present_scores(y_true, y_pred, n_bins = 5, normalized_sizes = True):\n",
    "\n",
    "    \"\"\"\n",
    "       A whole package for computing a various of metrics for binary classification,\n",
    "       It includes- f1 score for each class, PayPal coverage-precision curve, and roc-auc.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
    "            Ground truth (correct) target values.\n",
    "\n",
    "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
    "            Estimated targets probas for the positive class as returned by a classifier.\n",
    "\n",
    "        n_bins : int, optional.\n",
    "            The number of bins for vizualization PayPal coverage-precision curve. default is 5.\n",
    "\n",
    "        normalized_sizes : Boolean, optional.\n",
    "            Whether the axis of coverage will be in absolute numbers or not.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None, just printing and plotting.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame({\"predicted\": y_pred,\n",
    "                       \"actual\" : y_true})\n",
    "\n",
    "    df[\"prediction_group\"] = pd.cut(df[\"predicted\"], bins = np.linspace(0, 1, n_bins +1), include_lowest=True)\n",
    "    grouped = df.groupby(\"prediction_group\").size()\n",
    "    if normalized_sizes:\n",
    "        grouped = grouped/len(df)\n",
    "\n",
    "    ax1 = grouped.plot(kind=\"bar\", color = \"blue\")\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2 = df.groupby(\"prediction_group\")[\"actual\"].mean().plot(color = \"orange\")\n",
    "    ax2.axhline(y=y_true.mean(), xmin=0, xmax=1)\n",
    "    plt.show()\n",
    "    print(classification_report(y_true, y_pred > model_threshold))\n",
    "    print(\"AUC Score:\", roc_auc_score(y_true, y_pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "present_scores(y_test, y_pred_proba, n_bins=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compute confusion matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute confusion matrix to evaluate the accuracy of the classification - displaying actual total values numbers per category\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                               display_labels=target_names)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Compute confusion matrix to evaluate the accuracy of the classification - displaying normalized values numbers\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='all')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                               display_labels=target_names)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#create the profit function - based on the percentage of change in the \"market cap percentage\" column\n",
    "\n",
    "# create prediction column in the \"test\" dataframe based on the model results, set it to binary number instead of True / False\n",
    "test['prediction'] = y_pred.astype(int)\n",
    "# test = test.reset_index()\n",
    "test['profit_percentage'] = test['diff_in_mc_perc'] * test['prediction']\n",
    "profit_perc_sum = round(test['profit_percentage'].sum(), 3)\n",
    "profit_perc_mean = round(profit_perc_sum / test[test['prediction']==1].shape[0], 3)\n",
    "\n",
    "print(\"The total profit in percentage after investing in tested stock is:\",profit_perc_mean,\"%\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
